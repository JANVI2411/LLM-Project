{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":28785,"sourceType":"modelInstanceVersion","isSourceIdPinned":true,"modelInstanceId":8318,"modelId":3301}],"dockerImageVersionId":30887,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"%%capture\n!pip install unsloth\n!pip install --force-reinstall --no-cache-dir --no-deps git+https://github.com/unslothai/unsloth.git","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import torch","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-17T14:18:39.191350Z","iopub.execute_input":"2025-02-17T14:18:39.191595Z","iopub.status.idle":"2025-02-17T14:18:39.195231Z","shell.execute_reply.started":"2025-02-17T14:18:39.191574Z","shell.execute_reply":"2025-02-17T14:18:39.194109Z"}},"outputs":[],"execution_count":3},{"cell_type":"code","source":"from huggingface_hub import login\nfrom kaggle_secrets import UserSecretsClient\nuser_secrets = UserSecretsClient()\n\nhf_token = user_secrets.get_secret(\"HUGGINGFACE_TOKEN\")\nlogin(hf_token)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-17T14:18:39.195872Z","iopub.execute_input":"2025-02-17T14:18:39.196148Z","iopub.status.idle":"2025-02-17T14:18:39.417239Z","shell.execute_reply.started":"2025-02-17T14:18:39.196122Z","shell.execute_reply":"2025-02-17T14:18:39.416349Z"}},"outputs":[],"execution_count":4},{"cell_type":"code","source":"import wandb\n\nwb_token = user_secrets.get_secret(\"wandb\")\n\nwandb.login(key=wb_token)\nrun = wandb.init(\n    project='Fine-tune-gemma-2b-it on ultrafeedback_binarized Dataset', \n    job_type=\"training\", \n    anonymous=\"allow\"\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-17T14:18:39.420151Z","iopub.execute_input":"2025-02-17T14:18:39.420449Z","iopub.status.idle":"2025-02-17T14:18:54.572804Z","shell.execute_reply.started":"2025-02-17T14:18:39.420417Z","shell.execute_reply":"2025-02-17T14:18:54.572162Z"}},"outputs":[{"name":"stderr","text":"\u001b[34m\u001b[1mwandb\u001b[0m: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.\n\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mjanvi24\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m If you're specifying your api key in code, ensure this code is not shared publicly.\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Consider setting the WANDB_API_KEY environment variable, or running `wandb login` from the command line.\n\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Tracking run with wandb version 0.19.1"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Run data is saved locally in <code>/kaggle/working/wandb/run-20250217_141848-4dk8ek35</code>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Syncing run <strong><a href='https://wandb.ai/janvi24/Fine-tune-gemma-2b-it%20on%20ultrafeedback_binarized%20Dataset/runs/4dk8ek35' target=\"_blank\">toasty-sun-1</a></strong> to <a href='https://wandb.ai/janvi24/Fine-tune-gemma-2b-it%20on%20ultrafeedback_binarized%20Dataset' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View project at <a href='https://wandb.ai/janvi24/Fine-tune-gemma-2b-it%20on%20ultrafeedback_binarized%20Dataset' target=\"_blank\">https://wandb.ai/janvi24/Fine-tune-gemma-2b-it%20on%20ultrafeedback_binarized%20Dataset</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View run at <a href='https://wandb.ai/janvi24/Fine-tune-gemma-2b-it%20on%20ultrafeedback_binarized%20Dataset/runs/4dk8ek35' target=\"_blank\">https://wandb.ai/janvi24/Fine-tune-gemma-2b-it%20on%20ultrafeedback_binarized%20Dataset/runs/4dk8ek35</a>"},"metadata":{}}],"execution_count":5},{"cell_type":"code","source":"from unsloth import FastLanguageModel\n\nmax_seq_length = 2048 \ndtype = None \nload_in_4bit = True \n\nmodel, tokenizer = FastLanguageModel.from_pretrained(\n    model_name = \"/kaggle/input/gemma/transformers/2b-it/3\",\n    max_seq_length = max_seq_length,\n    dtype = dtype,\n    load_in_4bit = load_in_4bit,\n    token = hf_token, \n)\nmodel = FastLanguageModel.get_peft_model(model,\n                                         r=8, \n                                         target_modules=[\n                                            \"q_proj\",\n                                            \"k_proj\",\n                                            \"v_proj\",\n                                            \"o_proj\",\n                                            \"gate_proj\",\n                                            \"up_proj\",\n                                            \"down_proj\",\n                                        ],\n                                        lora_alpha=16,\n                                        lora_dropout=0,  \n                                        bias=\"none\",  \n                                        use_gradient_checkpointing=\"unsloth\",  # True or \"unsloth\" for very long context\n                                        random_state=3407,\n                                        use_rslora=False,  \n                                        loftq_config=None,\n                                    )","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-17T14:20:39.352459Z","iopub.execute_input":"2025-02-17T14:20:39.352751Z","iopub.status.idle":"2025-02-17T14:20:44.957874Z","shell.execute_reply.started":"2025-02-17T14:20:39.352729Z","shell.execute_reply":"2025-02-17T14:20:44.957096Z"}},"outputs":[{"name":"stderr","text":"Unsloth 2025.2.12 patched 18 layers with 18 QKV layers, 18 O layers and 18 MLP layers.\n","output_type":"stream"}],"execution_count":8},{"cell_type":"code","source":"from datasets import load_dataset\ntrain_dataset = load_dataset(\"trl-lib/ultrafeedback_binarized\", split=\"train[:500]\")\ntrain_dataset[0]","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-17T14:21:01.552050Z","iopub.execute_input":"2025-02-17T14:21:01.552439Z","iopub.status.idle":"2025-02-17T14:21:11.358901Z","shell.execute_reply.started":"2025-02-17T14:21:01.552404Z","shell.execute_reply":"2025-02-17T14:21:11.357986Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"README.md:   0%|          | 0.00/643 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"388d213e67ba48cfa228e655fe196dbf"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"train-00000-of-00001.parquet:   0%|          | 0.00/131M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"1220cc493a1e40979ad447a60b61bc1a"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"test-00000-of-00001.parquet:   0%|          | 0.00/2.14M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"1917937710a64654a10286e8e236ac23"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating train split:   0%|          | 0/62135 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b19f0c16a25f42b3af2b84cbe160700d"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating test split:   0%|          | 0/1000 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ee596bf37c2449c6b83d447968b04139"}},"metadata":{}},{"execution_count":9,"output_type":"execute_result","data":{"text/plain":"{'chosen': [{'content': 'Use the pygame library to write a version of the classic game Snake, with a unique twist',\n   'role': 'user'},\n  {'content': \"Sure, I'd be happy to help you write a version of the classic game Snake using the pygame library! Here's a basic outline of how we can approach this:\\n\\n1. First, we'll need to set up the game display and create a game object that we can use to handle the game's state.\\n2. Next, we'll create the game's grid, which will be used to represent the game board. We'll need to define the size of the grid and the spaces within it.\\n3. After that, we'll create the snake object, which will be used to represent the player's movement. We'll need to define the size of the snake and the speed at which it moves.\\n4. We'll also need to create a food object, which will be used to represent the food that the player must collect to score points. We'll need to define the location of the food and the speed at which it moves.\\n5. Once we have these objects set up, we can start handling the game's logic. We'll need to define the rules for the player's movements, such as how the player can move the snake and how the snake will grow as the player collects more food.\\n6. We'll also need to add collisions detection to the game, so that the snake and the food can collide with each other.\\n7. Finally, we'll need to add a user interface to the game, such as a menu and a scoreboard.\\n\\nNow, as for the unique twist, we could add a few features to make the game more interesting. For example, we could add power-ups that give the player special abilities, such as the ability to grow the snake faster or to change its direction. We could also add obstacles, such as walls or pits, that the player must avoid.\\n\\nHere's some sample code to get us started:\\n```\\nimport pygame\\n\\n# Initialize pygame\\npygame.init()\\n\\n# Set up the game display\\nwidth = 800\\nheight = 600\\nscreen = pygame.display.set_mode((width, height))\\n\\n# Define the colors\\nWHITE = (255, 255, 255)\\nBLACK = (0, 0, 0)\\nGREEN = (0, 255, 0)\\n\\n# Define the game objects\\nsnake = pygame.Rect(50, 50, 10, 10)\\nfood = pygame.Rect(100, 100, 10, 10)\\n\\n# Define the game logic\\ndef update_snake():\\n    # Get the mouse position\\n    mouse_pos = pygame.mouse.get_pos()\\n\\n    # Move the snake based on the mouse position\\n    if mouse_pos[0] > snake.x:\\n        snake.x += 10\\n    elif mouse_pos[0] < snake.x:\\n        snake.x -= 10\\n    elif mouse_pos[1] > snake.y:\\n        snake.y += 10\\n    elif mouse_pos[1] < snake.y:\\n        snake.y -= 10\\n\\n    # Update the snake's size\\n    if snake.x == food.x and snake.y == food.y:\\n        snake.width += 10\\n        snake.height += 10\\n\\n# Define the game loop\\ndef game_loop():\\n    # Clear the screen\\n    screen.fill(BLACK)\\n\\n    # Update the game objects\\n    update_snake()\\n\\n    # Draw the game objects\\n    screen.fill(WHITE)\\n    screen.draw.rect(snake, GREEN)\\n    screen.draw.rect(food, BLACK)\\n\\n    # Update the display\\n    pygame.display.flip()\\n\\n# Start the game loop\\ngame_loop()\\n\\n# Run the game\\nwhile True:\\n    for event in pygame.event.get():\\n        if event.type == pygame.QUIT:\\n            pygame.quit()\\n            break\\n\\n    pygame.time.Clock().tick(60)\\n```\\nThis code sets up a basic game display, defines the snake and food objects, and updates the game state based on the player's movements. We've also added a simple user interface and some basic collision detection.\\n\\nAs for the unique twist, we could add a few features to make the game\",\n   'role': 'assistant'}],\n 'rejected': [{'content': 'Use the pygame library to write a version of the classic game Snake, with a unique twist',\n   'role': 'user'},\n  {'content': 'Sure, here\\'s an example of how to write a version of Snake game with a unique twist using the Pygame library:\\n```python\\nimport pygame\\n\\nclass SnakeGame:\\n    def __init__(self, game_width, game_height):\\n        pygame.init()\\n        screen = pygame.display.set_mode((game_width, game_height))\\n        pygame.display.set_caption(\"Snake Game\")\\n        self.speed = 5  # Speed of the snake\\n        self.food_speed = 1  # Speed of the food\\n        self.direction = 0  # Initial direction of the snake\\n        self.snakelen = 0  # Length of the snake\\n        self.food = pygame.image.load(\"snake_food.png\")\\n        self.head = pygame.image.load(\"snake_head.png\")\\n        self.tail = pygame.image.load(\"snake_tail.png\")\\n        self.game Quint()\\n    def Quint(self):\\n        for i in range(50):\\n            pygame.draw.line(screen, (180, 100, 220), (0, 0, 200), 2)\\n            pygame.draw.line(screen, (180, 100, 220), (50, 0, 200), 2)\\n            pygame.draw.line(screen, (180, 100, 220), (50, 50, 200), 2)\\n            pygame.draw.line(screen, (180, 100, 220), (50, 50, 300), 2)\\n            pygame.draw.line(screen, (180, 100, 220), (50, 80, 200), 2)\\n            pygame.draw.line(screen, (180, 100, 220), (50, 80, 300), 2)\\n            pygame.draw.line(screen, (180, 100, 220), (0, 50, 200), 2)\\n            pygame.draw.line(screen, (180, 100, 220), (0, 50, 300), 2)\\n            pygame.draw.line(screen, (180, 100, 220), (0, 80, 200), 2)\\n            pygame.draw.line(screen, (180, 100, 220), (0, 80, 300), 2)\\n            pygame.display.flip()\\n        self.game.run()\\n    def run(self):\\n        while True:\\n            for event in pygame.event. pygame.KEYDOWN:\\n                if event.key == pygame.K_LEFT:\\n                    self.direction = -1\\n                if event.key == pygame.K_RIGHT:\\n                    self.direction = 1\\n            self.snakelen += 1\\n            if self.snakelen == 0:\\n                self.snakelen = 10\\n            if self.snakelen > 20:\\n                self.snakelen = 20\\n            self.gameQuint()\\n            self.foodCrossing()\\n            self.headRun()\\n            pygame.display.update()\\ngame = SnakeGame(800, 600)\\ngame.run()\\n```\\nIn this game, the snake moves with a constant speed, but the direction of the snake can be controlled by the user using the left and right arrow keys. The snake grows in length every 10 segments, and when it reaches a certain length, it resets to 10 segments. The food moves fast and randomly crosses the screen, and the snake can eat it by colliding with it. The snake\\'s head and tail move independently of each other. The game ends when the snake dies or reaches the end of the screen.',\n   'role': 'assistant'}],\n 'score_chosen': 6.0,\n 'score_rejected': 4.0}"},"metadata":{}}],"execution_count":9},{"cell_type":"code","source":"from trl import DPOConfig, DPOTrainer\nfrom unsloth import is_bfloat16_supported\n\ntraining_args = DPOConfig(per_device_train_batch_size=2,\n                        gradient_accumulation_steps=4,\n                        # Use num_train_epochs = 1, warmup_ratio for full training runs!\n                        warmup_steps=5,\n                        max_steps=5,\n                        learning_rate=2e-4,\n                        fp16=not is_bfloat16_supported(),\n                        bf16=is_bfloat16_supported(),\n                        logging_steps=1,\n                        optim=\"adamw_8bit\",\n                        weight_decay=0.01,\n                        lr_scheduler_type=\"linear\",\n                        seed=3407,\n                        output_dir=\"outputs\")\n\ntrainer = DPOTrainer(model=model, \n                     args=training_args, \n                     processing_class=tokenizer, \n                     train_dataset=train_dataset)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-17T14:24:25.672495Z","iopub.execute_input":"2025-02-17T14:24:25.673484Z","iopub.status.idle":"2025-02-17T14:24:25.862572Z","shell.execute_reply.started":"2025-02-17T14:24:25.673448Z","shell.execute_reply":"2025-02-17T14:24:25.861804Z"}},"outputs":[],"execution_count":19},{"cell_type":"code","source":"trainer.train()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-17T14:24:27.231514Z","iopub.execute_input":"2025-02-17T14:24:27.231810Z","iopub.status.idle":"2025-02-17T14:26:52.791283Z","shell.execute_reply.started":"2025-02-17T14:24:27.231788Z","shell.execute_reply":"2025-02-17T14:26:52.790567Z"}},"outputs":[{"name":"stderr","text":"==((====))==  Unsloth - 2x faster free finetuning | Num GPUs = 1\n   \\\\   /|    Num examples = 500 | Num Epochs = 1\nO^O/ \\_/ \\    Batch size per device = 2 | Gradient Accumulation steps = 4\n\\        /    Total batch size = 8 | Total steps = 10\n \"-____-\"     Number of trainable parameters = 9,805,824\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='10' max='10' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [10/10 02:09, Epoch 0/1]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Step</th>\n      <th>Training Loss</th>\n      <th>rewards / chosen</th>\n      <th>rewards / rejected</th>\n      <th>rewards / accuracies</th>\n      <th>rewards / margins</th>\n      <th>logps / chosen</th>\n      <th>logps / rejected</th>\n      <th>logits / chosen</th>\n      <th>logits / rejected</th>\n      <th>eval_logits / chosen</th>\n      <th>eval_logits / rejected</th>\n      <th>nll_loss</th>\n      <th>aux_loss</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>1</td>\n      <td>0.693100</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>-489.975464</td>\n      <td>-529.761597</td>\n      <td>-22.995388</td>\n      <td>-23.634632</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <td>2</td>\n      <td>0.693100</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>-643.203064</td>\n      <td>-503.523987</td>\n      <td>-22.909473</td>\n      <td>-22.580498</td>\n      <td>No Log</td>\n      <td>No Log</td>\n      <td>No Log</td>\n      <td>No Log</td>\n    </tr>\n    <tr>\n      <td>3</td>\n      <td>0.690100</td>\n      <td>0.016118</td>\n      <td>0.009942</td>\n      <td>0.625000</td>\n      <td>0.006176</td>\n      <td>-649.033081</td>\n      <td>-461.669556</td>\n      <td>-21.761307</td>\n      <td>-22.803642</td>\n      <td>No Log</td>\n      <td>No Log</td>\n      <td>No Log</td>\n      <td>No Log</td>\n    </tr>\n    <tr>\n      <td>4</td>\n      <td>0.632200</td>\n      <td>0.250476</td>\n      <td>0.113498</td>\n      <td>0.625000</td>\n      <td>0.136978</td>\n      <td>-698.312866</td>\n      <td>-520.073242</td>\n      <td>-23.086992</td>\n      <td>-22.621698</td>\n      <td>No Log</td>\n      <td>No Log</td>\n      <td>No Log</td>\n      <td>No Log</td>\n    </tr>\n    <tr>\n      <td>5</td>\n      <td>0.698600</td>\n      <td>0.276359</td>\n      <td>0.280949</td>\n      <td>0.375000</td>\n      <td>-0.004590</td>\n      <td>-457.854492</td>\n      <td>-521.314819</td>\n      <td>-25.344511</td>\n      <td>-23.820574</td>\n      <td>No Log</td>\n      <td>No Log</td>\n      <td>No Log</td>\n      <td>No Log</td>\n    </tr>\n    <tr>\n      <td>6</td>\n      <td>0.667000</td>\n      <td>0.690035</td>\n      <td>0.523746</td>\n      <td>0.500000</td>\n      <td>0.166289</td>\n      <td>-594.364990</td>\n      <td>-597.737183</td>\n      <td>-22.775448</td>\n      <td>-23.079523</td>\n      <td>No Log</td>\n      <td>No Log</td>\n      <td>No Log</td>\n      <td>No Log</td>\n    </tr>\n    <tr>\n      <td>7</td>\n      <td>0.745500</td>\n      <td>0.475509</td>\n      <td>0.499598</td>\n      <td>0.375000</td>\n      <td>-0.024089</td>\n      <td>-552.280579</td>\n      <td>-603.661865</td>\n      <td>-21.269972</td>\n      <td>-21.110535</td>\n      <td>No Log</td>\n      <td>No Log</td>\n      <td>No Log</td>\n      <td>No Log</td>\n    </tr>\n    <tr>\n      <td>8</td>\n      <td>0.695000</td>\n      <td>0.389698</td>\n      <td>0.255133</td>\n      <td>0.625000</td>\n      <td>0.134565</td>\n      <td>-603.745972</td>\n      <td>-509.499451</td>\n      <td>-22.787825</td>\n      <td>-24.457851</td>\n      <td>No Log</td>\n      <td>No Log</td>\n      <td>No Log</td>\n      <td>No Log</td>\n    </tr>\n    <tr>\n      <td>9</td>\n      <td>0.613300</td>\n      <td>0.118943</td>\n      <td>-0.072188</td>\n      <td>0.750000</td>\n      <td>0.191131</td>\n      <td>-308.556213</td>\n      <td>-394.061584</td>\n      <td>-20.878235</td>\n      <td>-21.048401</td>\n      <td>No Log</td>\n      <td>No Log</td>\n      <td>No Log</td>\n      <td>No Log</td>\n    </tr>\n    <tr>\n      <td>10</td>\n      <td>0.837900</td>\n      <td>0.293554</td>\n      <td>0.285103</td>\n      <td>0.375000</td>\n      <td>0.008451</td>\n      <td>-584.921875</td>\n      <td>-457.522583</td>\n      <td>-22.806589</td>\n      <td>-23.305445</td>\n      <td>No Log</td>\n      <td>No Log</td>\n      <td>No Log</td>\n      <td>No Log</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"execution_count":20,"output_type":"execute_result","data":{"text/plain":"TrainOutput(global_step=10, training_loss=0.6966004014015198, metrics={'train_runtime': 143.2304, 'train_samples_per_second': 0.559, 'train_steps_per_second': 0.07, 'total_flos': 0.0, 'train_loss': 0.6966004014015198, 'epoch': 0.16})"},"metadata":{}}],"execution_count":20},{"cell_type":"code","source":"if False:\n    model.push_to_hub_merged(\"rlhf-dpo-gemma-2b-it-model\", tokenizer, save_method = \"merged_16bit\")\n    # model.push_to_hub_gguf(\"rlhf-dpo-gemma-2b-it-model\", tokenizer, quantization_method = \"q8_0\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-17T14:36:08.791625Z","iopub.execute_input":"2025-02-17T14:36:08.791970Z","iopub.status.idle":"2025-02-17T14:38:31.544552Z","shell.execute_reply.started":"2025-02-17T14:36:08.791943Z","shell.execute_reply":"2025-02-17T14:38:31.543348Z"}},"outputs":[{"name":"stderr","text":"Unsloth: ##### The current model auto adds a BOS token.\nUnsloth: ##### Your chat template has a BOS token. We shall remove it temporarily.\n","output_type":"stream"},{"name":"stdout","text":"Cloning into 'llama.cpp'...\nSubmodule 'kompute' (https://github.com/nomic-ai/kompute.git) registered for path 'ggml/src/ggml-kompute/kompute'\nCloning into '/kaggle/working/llama.cpp/ggml/src/ggml-kompute/kompute'...\nSubmodule path 'ggml/src/ggml-kompute/kompute': checked out '4565194ed7c32d1d2efa32ceab4d3c6cae006306'\nRequirement already satisfied: gguf in /usr/local/lib/python3.10/dist-packages (0.14.0)\nRequirement already satisfied: protobuf in /usr/local/lib/python3.10/dist-packages (3.20.3)\nRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from gguf) (1.26.4)\nRequirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from gguf) (6.0.2)\nRequirement already satisfied: sentencepiece<=0.2.0,>=0.1.98 in /usr/local/lib/python3.10/dist-packages (from gguf) (0.2.0)\nRequirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from gguf) (4.67.1)\nRequirement already satisfied: mkl_fft in /usr/local/lib/python3.10/dist-packages (from numpy>=1.17->gguf) (1.3.8)\nRequirement already satisfied: mkl_random in /usr/local/lib/python3.10/dist-packages (from numpy>=1.17->gguf) (1.2.4)\nRequirement already satisfied: mkl_umath in /usr/local/lib/python3.10/dist-packages (from numpy>=1.17->gguf) (0.1.1)\nRequirement already satisfied: mkl in /usr/local/lib/python3.10/dist-packages (from numpy>=1.17->gguf) (2025.0.1)\nRequirement already satisfied: tbb4py in /usr/local/lib/python3.10/dist-packages (from numpy>=1.17->gguf) (2022.0.0)\nRequirement already satisfied: mkl-service in /usr/local/lib/python3.10/dist-packages (from numpy>=1.17->gguf) (2.4.1)\nRequirement already satisfied: intel-openmp>=2024 in /usr/local/lib/python3.10/dist-packages (from mkl->numpy>=1.17->gguf) (2024.2.0)\nRequirement already satisfied: tbb==2022.* in /usr/local/lib/python3.10/dist-packages (from mkl->numpy>=1.17->gguf) (2022.0.0)\nRequirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.10/dist-packages (from tbb==2022.*->mkl->numpy>=1.17->gguf) (1.2.0)\nRequirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.10/dist-packages (from mkl_umath->numpy>=1.17->gguf) (2024.2.0)\nRequirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.10/dist-packages (from intel-openmp>=2024->mkl->numpy>=1.17->gguf) (2024.2.0)\nmake: Entering directory '/kaggle/working/llama.cpp'\n-- The C compiler identification is GNU 11.4.0\n-- The CXX compiler identification is GNU 11.4.0\n-- Detecting C compiler ABI info\n-- Detecting C compiler ABI info - done\n-- Check for working C compiler: /usr/bin/cc - skipped\n-- Detecting C compile features\n-- Detecting C compile features - done\n-- Detecting CXX compiler ABI info\n-- Detecting CXX compiler ABI info - done\n-- Check for working CXX compiler: /usr/bin/c++ - skipped\n-- Detecting CXX compile features\n-- Detecting CXX compile features - done\n-- Found Git: /usr/bin/git (found version \"2.34.1\")\n-- Performing Test CMAKE_HAVE_LIBC_PTHREAD\n-- Performing Test CMAKE_HAVE_LIBC_PTHREAD - Success\n-- Found Threads: TRUE\n-- Warning: ccache not found - consider installing it for faster compilation or disable this warning with GGML_CCACHE=OFF\n-- CMAKE_SYSTEM_PROCESSOR: x86_64\n-- Including CPU backend\n-- Found OpenMP_C: -fopenmp (found version \"4.5\")\n-- Found OpenMP_CXX: -fopenmp (found version \"4.5\")\n-- Found OpenMP: TRUE (found version \"4.5\")\n-- x86 detected\n-- Adding CPU backend variant ggml-cpu: -march=native \n-- Found CURL: /usr/lib/x86_64-linux-gnu/libcurl.so (found version \"7.81.0\")\n-- Configuring done (2.0s)\n-- Generating done (0.3s)\n-- Build files have been written to: /kaggle/working/llama.cpp/build\n[  0%] Generating build details from Git\n[  0%] Building C object ggml/src/CMakeFiles/ggml-base.dir/ggml.c.o\n[  3%] Building C object ggml/src/CMakeFiles/ggml-base.dir/ggml-alloc.c.o\n[  7%] Building CXX object ggml/src/CMakeFiles/ggml-base.dir/ggml-opt.cpp.o\n-- Found Git: /usr/bin/git (found version \"2.34.1\")\n[  7%] Building CXX object ggml/src/CMakeFiles/ggml-base.dir/ggml-backend.cpp.o\n[  7%] Building CXX object ggml/src/CMakeFiles/ggml-base.dir/ggml-threading.cpp.o\n[  7%] Building CXX object ggml/src/CMakeFiles/ggml-base.dir/gguf.cpp.o\n[ 11%] Building C object ggml/src/CMakeFiles/ggml-base.dir/ggml-quants.c.o\n[ 11%] Building CXX object common/CMakeFiles/build_info.dir/build-info.cpp.o\n[ 11%] Built target build_info\n[ 15%] Linking CXX static library libggml-base.a\n[ 15%] Built target ggml-base\n[ 15%] Building C object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/ggml-cpu.c.o\n[ 19%] Building CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/ggml-cpu.cpp.o\n[ 19%] Building CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/ggml-cpu-aarch64.cpp.o\n[ 23%] Building CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/ggml-cpu-hbm.cpp.o\n[ 23%] Building C object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/ggml-cpu-quants.c.o\n[ 26%] Building CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/ggml-cpu-traits.cpp.o\n[ 30%] Building CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/amx/mmq.cpp.o\n[ 30%] Building CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/amx/amx.cpp.o\n[ 30%] Building CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/llamafile/sgemm.cpp.o\n[ 34%] Linking CXX static library libggml-cpu.a\n[ 34%] Built target ggml-cpu\n[ 34%] Building CXX object ggml/src/CMakeFiles/ggml.dir/ggml-backend-reg.cpp.o\n[ 38%] Linking CXX static library libggml.a\n[ 38%] Built target ggml\n[ 38%] Building CXX object src/CMakeFiles/llama.dir/llama.cpp.o\n[ 42%] Building CXX object src/CMakeFiles/llama.dir/llama-arch.cpp.o\n[ 42%] Building CXX object src/CMakeFiles/llama.dir/llama-adapter.cpp.o\n[ 42%] Building CXX object src/CMakeFiles/llama.dir/llama-batch.cpp.o\n[ 42%] Building CXX object src/CMakeFiles/llama.dir/llama-context.cpp.o\n[ 46%] Building CXX object src/CMakeFiles/llama.dir/llama-chat.cpp.o\n[ 50%] Building CXX object src/CMakeFiles/llama.dir/llama-grammar.cpp.o\n[ 50%] Building CXX object src/CMakeFiles/llama.dir/llama-hparams.cpp.o\n[ 53%] Building CXX object src/CMakeFiles/llama.dir/llama-impl.cpp.o\n[ 53%] Building CXX object src/CMakeFiles/llama.dir/llama-kv-cache.cpp.o\n[ 57%] Building CXX object src/CMakeFiles/llama.dir/llama-mmap.cpp.o\n[ 57%] Building CXX object src/CMakeFiles/llama.dir/llama-model-loader.cpp.o\n[ 61%] Building CXX object src/CMakeFiles/llama.dir/llama-model.cpp.o\n[ 61%] Building CXX object src/CMakeFiles/llama.dir/llama-quant.cpp.o\n[ 65%] Building CXX object src/CMakeFiles/llama.dir/llama-sampling.cpp.o\n[ 65%] Building CXX object src/CMakeFiles/llama.dir/llama-vocab.cpp.o\n[ 69%] Building CXX object src/CMakeFiles/llama.dir/unicode.cpp.o\n[ 69%] Building CXX object src/CMakeFiles/llama.dir/unicode-data.cpp.o\n[ 73%] Linking CXX static library libllama.a\n[ 73%] Built target llama\n[ 76%] Building CXX object common/CMakeFiles/common.dir/arg.cpp.o\n[ 76%] Building CXX object common/CMakeFiles/common.dir/chat.cpp.o\n[ 80%] Building CXX object common/CMakeFiles/common.dir/common.cpp.o\n[ 80%] Building CXX object common/CMakeFiles/common.dir/console.cpp.o\n[ 84%] Building CXX object common/CMakeFiles/common.dir/json-schema-to-grammar.cpp.o\n[ 84%] Building CXX object common/CMakeFiles/common.dir/llguidance.cpp.o\n[ 88%] Building CXX object common/CMakeFiles/common.dir/log.cpp.o\n[ 88%] Building CXX object common/CMakeFiles/common.dir/ngram-cache.cpp.o\n[ 92%] Building CXX object common/CMakeFiles/common.dir/sampling.cpp.o\n[ 92%] Building CXX object common/CMakeFiles/common.dir/speculative.cpp.o\n[ 96%] Linking CXX static library libcommon.a\n[ 96%] Built target common\n[ 96%] Building CXX object examples/quantize/CMakeFiles/llama-quantize.dir/quantize.cpp.o\n[100%] Linking CXX executable ../../bin/llama-quantize\n[100%] Built target llama-quantize\n[  0%] Built target build_info\n[ 16%] Built target ggml-base\n[ 36%] Built target ggml-cpu\n[ 40%] Built target ggml\n[ 76%] Built target llama\n[100%] Built target common\n[100%] Building CXX object examples/export-lora/CMakeFiles/llama-export-lora.dir/export-lora.cpp.o\n[100%] Linking CXX executable ../../bin/llama-export-lora\n[100%] Built target llama-export-lora\n[  0%] Built target build_info\n[ 15%] Built target ggml-base\n[ 34%] Built target ggml-cpu\n[ 38%] Built target ggml\n[ 73%] Built target llama\n[ 96%] Built target common\n[ 96%] Building CXX object examples/main/CMakeFiles/llama-cli.dir/main.cpp.o\n[100%] Linking CXX executable ../../bin/llama-cli\n[100%] Built target llama-cli\nUnsloth: Merging 4bit and LoRA weights to 16bit...\nUnsloth: Will use up to 18.07 out of 31.35 RAM for saving.\nUnsloth: Saving model... This might take 5 minutes ...\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 18/18 [00:00<00:00, 26.20it/s]\n","output_type":"stream"},{"name":"stdout","text":"Unsloth: Saving tokenizer... Done.\nUnsloth: Saving rlhf-dpo-gemma-2b-it-model/pytorch_model-00001-of-00002.bin...\nUnsloth: Saving rlhf-dpo-gemma-2b-it-model/pytorch_model-00002-of-00002.bin...\nDone.\n","output_type":"stream"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mOSError\u001b[0m                                   Traceback (most recent call last)","\u001b[0;32m<ipython-input-24-9bc582f15654>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m     \u001b[0;31m# model.push_to_hub_merged(\"rlhf-dpo-gemma-2b-it-model\", tokenizer, save_method = \"merged_16bit\")\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m     \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpush_to_hub_gguf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"rlhf-dpo-gemma-2b-it-model\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mquantization_method\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"q8_0\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/unsloth/save.py\u001b[0m in \u001b[0;36munsloth_push_to_hub_gguf\u001b[0;34m(self, repo_id, tokenizer, quantization_method, first_conversion, use_temp_dir, commit_message, private, token, max_shard_size, create_pr, safe_serialization, revision, commit_description, tags, temporary_location, maximum_memory_usage)\u001b[0m\n\u001b[1;32m   1908\u001b[0m     \u001b[0;32mpass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1909\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1910\u001b[0;31m     \u001b[0mis_sentencepiece_model\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcheck_if_sentencepiece_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1911\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1912\u001b[0m     \u001b[0;31m# Save to GGUF\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/unsloth/save.py\u001b[0m in \u001b[0;36mcheck_if_sentencepiece_model\u001b[0;34m(model, temporary_location)\u001b[0m\n\u001b[1;32m    117\u001b[0m         \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmakedirs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile_location\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    118\u001b[0m     \u001b[0;32mpass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 119\u001b[0;31m     \u001b[0mtemp_tokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave_pretrained\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile_location\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    120\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0misfile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"{file_location}/tokenizer.model\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    121\u001b[0m         \u001b[0msentencepiece_model\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py\u001b[0m in \u001b[0;36msave_pretrained\u001b[0;34m(self, save_directory, legacy_format, filename_prefix, push_to_hub, **kwargs)\u001b[0m\n\u001b[1;32m   2486\u001b[0m             \u001b[0mtokenizer_config\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"device_map\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2487\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2488\u001b[0;31m         \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtokenizer_config_file\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"w\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"utf-8\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2489\u001b[0m             \u001b[0mout_str\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mjson\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdumps\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtokenizer_config\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindent\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msort_keys\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mensure_ascii\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m\"\\n\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2490\u001b[0m             \u001b[0mf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout_str\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mOSError\u001b[0m: [Errno 30] Read-only file system: '/kaggle/input/gemma/transformers/2b-it/3/tokenizer_config.json'"],"ename":"OSError","evalue":"[Errno 30] Read-only file system: '/kaggle/input/gemma/transformers/2b-it/3/tokenizer_config.json'","output_type":"error"}],"execution_count":24},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}