{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":11413,"sourceType":"modelInstanceVersion","isSourceIdPinned":true,"modelInstanceId":6206,"modelId":3301}],"dockerImageVersionId":30887,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"%%capture\n!pip install unsloth\n# Also get the latest nightly Unsloth!\n!pip install --force-reinstall --no-cache-dir --no-deps git+https://github.com/unslothai/unsloth.git\n!pip install mlflow\n!pip install evaluate\n!pip install rouge_score\n!pip install bert_score","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-02-20T14:41:23.192004Z","iopub.execute_input":"2025-02-20T14:41:23.192319Z","iopub.status.idle":"2025-02-20T14:45:03.151887Z","shell.execute_reply.started":"2025-02-20T14:41:23.192292Z","shell.execute_reply":"2025-02-20T14:45:03.150967Z"}},"outputs":[],"execution_count":1},{"cell_type":"code","source":"from huggingface_hub import login\nfrom kaggle_secrets import UserSecretsClient\nuser_secrets = UserSecretsClient()\n\nhf_token = user_secrets.get_secret(\"HUGGINGFACE_TOKEN\")\nlogin(hf_token)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-20T14:45:16.977030Z","iopub.execute_input":"2025-02-20T14:45:16.977332Z","iopub.status.idle":"2025-02-20T14:45:18.293730Z","shell.execute_reply.started":"2025-02-20T14:45:16.977309Z","shell.execute_reply":"2025-02-20T14:45:18.292849Z"}},"outputs":[],"execution_count":2},{"cell_type":"code","source":"dataset = load_dataset(\"openlifescienceai/medmcqa\", split=\"validation\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-20T17:37:51.028322Z","iopub.execute_input":"2025-02-20T17:37:51.028633Z","iopub.status.idle":"2025-02-20T17:38:05.415750Z","shell.execute_reply.started":"2025-02-20T17:37:51.028609Z","shell.execute_reply":"2025-02-20T17:38:05.415161Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"README.md:   0%|          | 0.00/10.7k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"2c4503881d1f4b93b3efcede877675b1"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"train-00000-of-00001.parquet:   0%|          | 0.00/85.9M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"8dc367af238b4e79afb1400a38352249"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"test-00000-of-00001.parquet:   0%|          | 0.00/936k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"2392a3212f2b4935ae651dea298381f6"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"validation-00000-of-00001.parquet:   0%|          | 0.00/1.48M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"10e32e7bea0941c582e83ed883b8651b"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating train split:   0%|          | 0/182822 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"bb2b712a029445dcb3d9093285db7c76"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating test split:   0%|          | 0/6150 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"593d0548288b4f0895d4a51584856798"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating validation split:   0%|          | 0/4183 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"43bed2462dae4815b7abcb5e68b3c253"}},"metadata":{}}],"execution_count":67},{"cell_type":"code","source":"dataset","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-20T17:38:45.388041Z","iopub.execute_input":"2025-02-20T17:38:45.388349Z","iopub.status.idle":"2025-02-20T17:38:45.393511Z","shell.execute_reply.started":"2025-02-20T17:38:45.388324Z","shell.execute_reply":"2025-02-20T17:38:45.392645Z"}},"outputs":[{"execution_count":71,"output_type":"execute_result","data":{"text/plain":"Dataset({\n    features: ['id', 'question', 'opa', 'opb', 'opc', 'opd', 'cop', 'choice_type', 'exp', 'subject_name', 'topic_name'],\n    num_rows: 4183\n})"},"metadata":{}}],"execution_count":71},{"cell_type":"code","source":"dataset[\"question\"][0]","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-20T17:40:13.231672Z","iopub.execute_input":"2025-02-20T17:40:13.232016Z","iopub.status.idle":"2025-02-20T17:40:13.241271Z","shell.execute_reply.started":"2025-02-20T17:40:13.231990Z","shell.execute_reply":"2025-02-20T17:40:13.240404Z"}},"outputs":[{"execution_count":77,"output_type":"execute_result","data":{"text/plain":"'Which of the following is not true for myelinated nerve fibers:'"},"metadata":{}}],"execution_count":77},{"cell_type":"code","source":"# Define valid answer choices\nvalid_choices = [\"1\", \"2\", \"3\", \"4\"]\nvalid_tokens = tokenizer(valid_choices, add_special_tokens=False)[\"input_ids\"]  # Get token IDs for A, B, C, D\n\nprompt_temp =  \"\"\"Below is an instruction that describes a task, paired with an input that provides further context. \nWrite a response that appropriately completes the request. \nBefore answering, think carefully about the question and create a step-by-step chain of thoughts to ensure a logical and accurate response.\n\n### Instruction:\nYou are a medical expert with advanced knowledge in clinical reasoning, diagnostics, and treatment planning. \nPlease answer the following medical question. \n\n### Question:\n{}\n\n### Response:\n<think>{}\"\"\"\n\ndef generate_mcqa_answer(model, tokenizer, question, options):\n    # prompt = f\"\"\"###Instruction: Choose the correct option (1, 2, 3 or 4) for the following question.\\n\\n\n    #              ###Question: {question} \\n\\nOptions:\\n\"\"\"\n    prompt = prompt_temp.format(question,options[0],options[1],options[2],options[3],\"\")\n    inputs = tokenizer(prompt, return_tensors=\"pt\").to(\"cuda\")\n\n    with torch.no_grad():\n        output = model.generate(\n            **inputs, max_new_tokens=1, eos_token_id=tokenizer.eos_token_id\n        )\n\n    # Extract predicted token\n    predicted_text = tokenizer.decode(output[0], skip_special_tokens=True).strip()\n    for choice in valid_choices:\n        if choice in predicted_text:\n            return choice\n    return \"INVALID\"\n\n# Run Evaluation\npredictions, references = [], []\n\nfor i in range(100):  # Evaluate on 100 samples for speed\n    question = dataset[i][\"question\"]\n    options = [dataset[i][\"opa\"], dataset[i][\"opb\"], dataset[i][\"opc\"], dataset[i][\"opd\"]]\n    correct_choice = str(dataset[i][\"cop\"])  # Convert label to string\n    \n    predicted_choice = generate_mcqa_answer(model, tokenizer, question, options)\n    if predicted_choice in valid_choices:  # Only count valid predictions\n        predictions.append(predicted_choice)\n        references.append(correct_choice)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Compute Accuracy\nfrom sklearn.metrics import accuracy_score\naccuracy = accuracy_score(references, predictions)*100\nprint(f\"Model Accuracy: {accuracy:.2f}\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# print(rouge_scores)\n# print(bleu_score)\n# print(P)\n# print(R)\n# print(F1)\n# # print(meteor_score)\n# # {'rouge1': 0.39861943638797637, 'rouge2': 0.15852988222338912, 'rougeL': 0.24056601785296156, 'rougeLsum': 0.25601796407185623}\n# # {'bleu': 0.10663558385629832, 'precisions': [0.477088948787062, 0.18032786885245902, 0.0997229916897507, 0.06460674157303371], 'brevity_penalty': 0.6949736126020096, 'length_ratio': 0.733201581027668, 'translation_length': 371, 'reference_length': 506}\n# # tensor([0.5492, 0.7260, 0.7357, 0.6497, 0.5720])\n# # tensor([0.5879, 0.5425, 0.6210, 0.6112, 0.5811])\n# # tensor([0.5679, 0.6210, 0.6735, 0.6298, 0.5765])","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Evaluation Loop","metadata":{}},{"cell_type":"code","source":"import mlflow\nimport torch\nfrom unsloth import FastLanguageModel\nfrom transformers import AutoTokenizer\nfrom datasets import load_dataset\nfrom torch.utils.data import DataLoader\nfrom tqdm import tqdm\nimport evaluate\nimport numpy as np\nimport math\nfrom torch.nn.functional import cross_entropy","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-20T14:45:24.472059Z","iopub.execute_input":"2025-02-20T14:45:24.472455Z","iopub.status.idle":"2025-02-20T14:45:53.720823Z","shell.execute_reply.started":"2025-02-20T14:45:24.472423Z","shell.execute_reply":"2025-02-20T14:45:53.719956Z"}},"outputs":[{"name":"stdout","text":"🦥 Unsloth: Will patch your computer to enable 2x faster free finetuning.\n🦥 Unsloth Zoo will now patch everything to make training faster!\n","output_type":"stream"}],"execution_count":3},{"cell_type":"code","source":"max_seq_length = 2048 \ndtype = None \nload_in_4bit = True ","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-20T14:45:53.721909Z","iopub.execute_input":"2025-02-20T14:45:53.722147Z","iopub.status.idle":"2025-02-20T14:45:53.725959Z","shell.execute_reply.started":"2025-02-20T14:45:53.722126Z","shell.execute_reply":"2025-02-20T14:45:53.724875Z"}},"outputs":[],"execution_count":4},{"cell_type":"code","source":"prompt_style = \"\"\"Below is an instruction that describes a task, paired with an input that provides further context. \nWrite a response that appropriately completes the request. \nBefore answering, think carefully about the question and create a step-by-step chain of thoughts to ensure a logical and accurate response.\n\n### Instruction:\nYou are a medical expert with advanced knowledge in clinical reasoning, diagnostics, and treatment planning. \nPlease answer the following medical question. \n\n### Question:\n{}\n\n### Response:\n<think>{}\"\"\"\n\neval_dataset = load_dataset(\"JC-24/MediQAReasoning\", split=\"validation[:100]\")\n# dataloader = DataLoader(eval_dataset, batch_size=1)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-20T16:09:48.932129Z","iopub.execute_input":"2025-02-20T16:09:48.932460Z","iopub.status.idle":"2025-02-20T16:09:52.874900Z","shell.execute_reply.started":"2025-02-20T16:09:48.932436Z","shell.execute_reply":"2025-02-20T16:09:52.873984Z"}},"outputs":[],"execution_count":61},{"cell_type":"code","source":"model_name_or_path = \"JC-24/gemma-7b-mediqa-final\"\nfrom unsloth import is_bfloat16_supported\n\ndtype = torch.bfloat16 if is_bfloat16_supported() else torch.float16\nprint(dtype)\nmodel, tokenizer = FastLanguageModel.from_pretrained(\n    model_name_or_path,\n    load_in_4bit=load_in_4bit,  # Enable 4-bit quantization\n    max_seq_length=max_seq_length,\n    dtype=dtype\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-20T14:46:39.551779Z","iopub.execute_input":"2025-02-20T14:46:39.552106Z","iopub.status.idle":"2025-02-20T14:57:46.436639Z","shell.execute_reply.started":"2025-02-20T14:46:39.552081Z","shell.execute_reply":"2025-02-20T14:57:46.435975Z"}},"outputs":[{"name":"stdout","text":"torch.float16\n==((====))==  Unsloth 2025.2.14: Fast Gemma patching. Transformers: 4.49.0.\n   \\\\   /|    GPU: Tesla T4. Max memory: 14.741 GB. Platform: Linux.\nO^O/ \\_/ \\    Torch: 2.6.0+cu124. CUDA: 7.5. CUDA Toolkit: 12.4. Triton: 3.2.0\n\\        /    Bfloat16 = FALSE. FA [Xformers = 0.0.29.post3. FA2 = False]\n \"-____-\"     Free Apache license: http://github.com/unslothai/unsloth\nUnsloth: Fast downloading is enabled - ignore downloading bars which are red colored!\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"pytorch_model.bin.index.json:   0%|          | 0.00/20.9k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"fffe55e827f343788f8e3a68e0344753"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading shards:   0%|          | 0/4 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"44be01e244f34768893470fe4bf77e6f"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"pytorch_model-00001-of-00004.bin:   0%|          | 0.00/5.00G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"9d13cba562ee477eba3fe3b1f68a4b85"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model.safetensors.index.json:   0%|          | 0.00/21.9k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"89bc44d07f9f4c8e9939b636aefc47e1"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"pytorch_model-00002-of-00004.bin:   0%|          | 0.00/4.98G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e7c3f5b9bc8c422ca839302d0c07fb65"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"pytorch_model-00003-of-00004.bin:   0%|          | 0.00/4.98G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"04672e4d8a8b4292bcc9da7a8cdcd489"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"pytorch_model-00004-of-00004.bin:   0%|          | 0.00/2.11G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"50e29d37bec64d8794b632cbe2af6df4"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"38b87c6638084785b397262ac3156fdf"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"generation_config.json:   0%|          | 0.00/154 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"2fd99382e25047499399097e79c702ed"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/1.16k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"7f9be31f0435424b855d5d1a534969a4"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.model:   0%|          | 0.00/4.24M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"95fecf1ea40a49eeb2d34ee03c054243"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"special_tokens_map.json:   0%|          | 0.00/555 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"6c4ed290b203401ea804fff6ddbadae1"}},"metadata":{}}],"execution_count":6},{"cell_type":"code","source":"FastLanguageModel.for_inference(model)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-20T14:57:56.352469Z","iopub.execute_input":"2025-02-20T14:57:56.352781Z","iopub.status.idle":"2025-02-20T14:57:56.360211Z","shell.execute_reply.started":"2025-02-20T14:57:56.352756Z","shell.execute_reply":"2025-02-20T14:57:56.359448Z"}},"outputs":[{"execution_count":7,"output_type":"execute_result","data":{"text/plain":"GemmaForCausalLM(\n  (model): GemmaModel(\n    (embed_tokens): Embedding(256000, 3072, padding_idx=0)\n    (layers): ModuleList(\n      (0-27): 28 x GemmaDecoderLayer(\n        (self_attn): GemmaAttention(\n          (q_proj): Linear4bit(in_features=3072, out_features=4096, bias=False)\n          (k_proj): Linear4bit(in_features=3072, out_features=4096, bias=False)\n          (v_proj): Linear4bit(in_features=3072, out_features=4096, bias=False)\n          (o_proj): Linear4bit(in_features=4096, out_features=3072, bias=False)\n          (rotary_emb): GemmaFixedRotaryEmbedding()\n        )\n        (mlp): GemmaMLP(\n          (gate_proj): Linear4bit(in_features=3072, out_features=24576, bias=False)\n          (up_proj): Linear4bit(in_features=3072, out_features=24576, bias=False)\n          (down_proj): Linear4bit(in_features=24576, out_features=3072, bias=False)\n          (act_fn): GELUActivation()\n        )\n        (input_layernorm): GemmaRMSNorm((3072,), eps=1e-06)\n        (post_attention_layernorm): GemmaRMSNorm((3072,), eps=1e-06)\n      )\n    )\n    (norm): GemmaRMSNorm((3072,), eps=1e-06)\n    (rotary_emb): GemmaFixedRotaryEmbedding()\n  )\n  (lm_head): Linear(in_features=3072, out_features=256000, bias=False)\n)"},"metadata":{}}],"execution_count":7},{"cell_type":"code","source":"eval_dataset","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-20T15:34:11.067165Z","iopub.execute_input":"2025-02-20T15:34:11.067470Z","iopub.status.idle":"2025-02-20T15:34:11.072620Z","shell.execute_reply.started":"2025-02-20T15:34:11.067448Z","shell.execute_reply":"2025-02-20T15:34:11.071694Z"}},"outputs":[{"execution_count":41,"output_type":"execute_result","data":{"text/plain":"Dataset({\n    features: ['Question', 'Complex_CoT', 'Response'],\n    num_rows: 5\n})"},"metadata":{}}],"execution_count":41},{"cell_type":"code","source":"all_preds = []\nall_refs = []\nall_pred_cot = []\nall_cot = []\nall_questions = []\n# Load Hugging Face Evaluation Metrics\nbleu_metric = evaluate.load(\"bleu\")\nrouge_metric = evaluate.load(\"rouge\")\n# meteor_metric = evaluate.load(\"meteor\")\nbertscore_metric = evaluate.load(\"bertscore\")\nexact_match_metric = evaluate.load(\"exact_match\")\n\nfor i in range(len(eval_dataset)):\n    question = eval_dataset[i][\"Question\"]\n    answer = eval_dataset[i][\"Response\"]\n    cot = eval_dataset[i][\"Complex_CoT\"]\n    \n    inputs = tokenizer([prompt_style.format(question, \"\")], return_tensors=\"pt\").to(\"cuda\")\n\n    outputs = model.generate(\n        input_ids=inputs.input_ids,\n        attention_mask=inputs.attention_mask,\n        max_new_tokens=1200,\n        use_cache=True,\n    )\n    response = tokenizer.batch_decode(outputs)\n    clear_res = response[0].split(\"</think>\")[-1]\n    pred_cot = response[0].split(\"<think>\")[-1].split(\"</think>\")[0]\n    all_questions.append(question)\n    all_preds.append(clear_res)\n    all_refs.append(answer)\n    all_pred_cot.append(pred_cot)\n    all_cot.append(cot)\n    \nbleu = bleu_metric.compute(predictions=all_preds, references=all_refs)['bleu']\nrouge = rouge_metric.compute(predictions=all_preds, references=all_refs)\nbertscore = bertscore_metric.compute(predictions=all_preds, references=all_refs, lang=\"en\")\nexact_match = exact_match_metric.compute(predictions=all_preds, references=all_refs)['exact_match']\n\nmetrics = {\n        # \"loss\": avg_loss,\n        # \"perplexity\": perplexity,\n        \"bleu\": bleu,\n        # \"meteor\": meteor,\n        \"exact_match\": exact_match,\n        \"rouge1\": rouge[\"rouge1\"],\n        \"rouge2\": rouge[\"rouge2\"],\n        \"rougeL\": rouge[\"rougeL\"],\n        \"bertscore_precision\": np.mean(bertscore[\"precision\"]),\n        \"bertscore_recall\": np.mean(bertscore[\"recall\"]),\n        \"bertscore_f1\": np.mean(bertscore[\"f1\"]),\n    }","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-20T16:11:23.781459Z","iopub.execute_input":"2025-02-20T16:11:23.781811Z","iopub.status.idle":"2025-02-20T17:10:51.927395Z","shell.execute_reply.started":"2025-02-20T16:11:23.781771Z","shell.execute_reply":"2025-02-20T17:10:51.926651Z"}},"outputs":[{"name":"stderr","text":"Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","output_type":"stream"}],"execution_count":64},{"cell_type":"code","source":"print(metrics)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-20T17:10:51.928502Z","iopub.execute_input":"2025-02-20T17:10:51.928797Z","iopub.status.idle":"2025-02-20T17:10:51.933241Z","shell.execute_reply.started":"2025-02-20T17:10:51.928766Z","shell.execute_reply":"2025-02-20T17:10:51.932344Z"}},"outputs":[{"name":"stdout","text":"{'bleu': 0.1586626140636257, 'exact_match': 0.0, 'rouge1': 0.46471873674751546, 'rouge2': 0.24126687188185544, 'rougeL': 0.32993377246628985, 'bertscore_precision': 0.889073098897934, 'bertscore_recall': 0.8800099664926528, 'bertscore_f1': 0.8841670423746109}\n","output_type":"stream"}],"execution_count":65},{"cell_type":"code","source":"import pandas as pd\nimport dagshub\nimport mlflow\n\ndagshub.init(repo_owner='janvichokshi1998', repo_name='gemma-7b-unsloth-mediQA', mlflow=True)\nmlflow.set_tracking_uri(\"https://dagshub.com/janvichokshi1998/gemma-7b-unsloth-mediQA.mlflow\")\n# mlflow.create_experiment(\"Gemma-7B-Unsloth-4bit-HF-Eval\")\nmlflow.set_experiment(\"Gemma-7B-Unsloth-4bit-HF-Eval\")\n\nwith mlflow.start_run(run_name=\"Gemma-7B-Unsloth-4bit-HF-Eval\") as run:\n    # Log all metrics\n    for key, value in metrics.items():\n        mlflow.log_metric(key, value)\n\n    # Log parameters\n    mlflow.log_param(\"model_name\", model_name_or_path)\n    mlflow.log_param(\"quantization\", \"4-bit\")\n    mlflow.log_param(\"batch_size\", 4)\n\n    example_data = {\n    \"Question\": all_questions,\n    \"Predicted Answer\": all_preds,\n    \"Actual Answer\": all_refs,\n    \"Predicted CoT\": all_pred_cot,\n    \"Actual CoT\": all_cot\n    }\n    \n    # Convert to DataFrame\n    examples_df = pd.DataFrame(example_data)\n    csv_save_path = \"/kaggle/working/example_predictions.csv\"\n    examples_df.to_csv(csv_save_path, index=False)\n    \n    # Log CSV file to MLflow and DagsHub\n    mlflow.log_artifact(csv_save_path)\n    print(f\"Logged to MLflow Run ID: {run.info.run_id}\")\n    print(\"Evaluation Metrics:\")\n    for key, value in metrics.items():\n        print(f\"{key}: {value:.4f}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-20T17:11:52.713869Z","iopub.execute_input":"2025-02-20T17:11:52.714206Z","iopub.status.idle":"2025-02-20T17:11:58.138929Z","shell.execute_reply.started":"2025-02-20T17:11:52.714182Z","shell.execute_reply":"2025-02-20T17:11:58.138215Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"Initialized MLflow to track repo \u001b[32m\"janvichokshi1998/gemma-7b-unsloth-mediQA\"\u001b[0m\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Initialized MLflow to track repo <span style=\"color: #008000; text-decoration-color: #008000\">\"janvichokshi1998/gemma-7b-unsloth-mediQA\"</span>\n</pre>\n"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Repository janvichokshi1998/gemma-7b-unsloth-mediQA initialized!\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Repository janvichokshi1998/gemma-7b-unsloth-mediQA initialized!\n</pre>\n"},"metadata":{}},{"name":"stdout","text":"Logged to MLflow Run ID: 3fb5894903104778a5b81031562306fd\nEvaluation Metrics:\nbleu: 0.1587\nexact_match: 0.0000\nrouge1: 0.4647\nrouge2: 0.2413\nrougeL: 0.3299\nbertscore_precision: 0.8891\nbertscore_recall: 0.8800\nbertscore_f1: 0.8842\n🏃 View run Gemma-7B-Unsloth-4bit-HF-Eval at: https://dagshub.com/janvichokshi1998/gemma-7b-unsloth-mediQA.mlflow/#/experiments/1/runs/3fb5894903104778a5b81031562306fd\n🧪 View experiment at: https://dagshub.com/janvichokshi1998/gemma-7b-unsloth-mediQA.mlflow/#/experiments/1\n","output_type":"stream"}],"execution_count":66},{"cell_type":"code","source":"# # Load Hugging Face Evaluation Metrics\n# bleu_metric = evaluate.load(\"bleu\")\n# rouge_metric = evaluate.load(\"rouge\")\n# # meteor_metric = evaluate.load(\"meteor\")\n# bertscore_metric = evaluate.load(\"bertscore\")\n# exact_match_metric = evaluate.load(\"exact_match\")\n\n \n# # Evaluation Function Using Hugging Face Metrics\n# def evaluate_model(model, dataloader):\n#     total_loss = 0.0\n#     total_tokens = 0\n\n#     all_preds = []\n#     all_refs = []\n#     device = \"cuda\"\n    \n#     with torch.no_grad():\n#         for batch in tqdm(dataloader, desc=\"Evaluating\"):\n#             prompt = prompt_style.format(batch['Question'][0],\"\")\n#             print(prompt)\n#             inputs = tokenizer(prompt, padding=\"max_length\", max_length = max_seq_length,truncation=True, return_tensors=\"pt\").to(\"cuda\")\n#             # outputs = model(input_ids=input_ids, attention_mask=attention_mask, labels=input_ids)\n#             # logits = outputs.logits\n#             # shift_logits = logits[..., :-1, :].contiguous()\n#             # shift_labels = input_ids[..., 1:].contiguous()\n    \n#             # # Compute loss manually to control masking\n#             # loss = cross_entropy(\n#             #     shift_logits.view(-1, shift_logits.size(-1)),\n#             #     shift_labels.view(-1),\n#             #     ignore_index=tokenizer.pad_token_id,\n#             #     reduction='sum'  # Sum to get total loss\n#             # )\n\n#             # # Count non-padding tokens\n#             # non_pad_tokens = (shift_labels != tokenizer.pad_token_id).sum().item()\n    \n#             # total_loss += loss.item()\n#             # total_tokens += non_pad_tokens\n            \n#             # Generate predictions\n#             outputs = model.generate(\n#                                     input_ids=inputs.input_ids,\n#                                     attention_mask=inputs.attention_mask,\n#                                     max_new_tokens=1200,\n#                                     use_cache=True,\n#                                 )\n#             decoded_preds = tokenizer.batch_decode(outputs)\n#             # generated_ids = model.generate(input_ids=input_ids, attention_mask=attention_mask, max_new_tokens=1200, use_cache=True)\n#             # decoded_preds = tokenizer.batch_decode(generated_ids, skip_special_tokens=True)\n#             decoded_pred_new = [decoded_pred.split(\"</think>\")[-1] for decoded_pred in decoded_preds]\n#             print(decoded_pred_new)\n#             break\n#             # print(\"------------------\")\n#             # actual_response = [res for res in batch[\"Response\"]]\n#             # print(actual_response)\n#             # all_preds.extend(decoded_pred_new)\n#             # all_refs.extend(actual_response)\n\n#     # Calculate Metrics\n#     # avg_loss = total_loss / total_tokens\n#     # perplexity = math.exp(avg_loss)\n\n#     # Compute Metrics using Hugging Face Evaluate\n#     # bleu = bleu_metric.compute(predictions=all_preds, references=all_refs)['bleu']\n#     # rouge = rouge_metric.compute(predictions=all_preds, references=all_refs)\n#     # # meteor = meteor_metric.compute(predictions=all_preds, references=all_refs)['meteor']\n#     # bertscore = bertscore_metric.compute(predictions=all_preds, references=all_refs, lang=\"en\")\n#     # exact_match = exact_match_metric.compute(predictions=all_preds, references=all_refs)['exact_match']\n\n#     # metrics = {\n#     #     # \"loss\": avg_loss,\n#     #     # \"perplexity\": perplexity,\n#     #     \"bleu\": bleu,\n#     #     # \"meteor\": meteor,\n#     #     \"exact_match\": exact_match,\n#     #     \"rouge1\": rouge[\"rouge1\"],\n#     #     \"rouge2\": rouge[\"rouge2\"],\n#     #     \"rougeL\": rouge[\"rougeL\"],\n#     #     \"bertscore_precision\": np.mean(bertscore[\"precision\"]),\n#     #     \"bertscore_recall\": np.mean(bertscore[\"recall\"]),\n#     #     \"bertscore_f1\": np.mean(bertscore[\"f1\"]),\n#     # }\n#     # return metrics, all_preds, all_refs","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-20T15:25:30.747483Z","iopub.execute_input":"2025-02-20T15:25:30.747775Z","iopub.status.idle":"2025-02-20T15:25:36.490167Z","shell.execute_reply.started":"2025-02-20T15:25:30.747753Z","shell.execute_reply":"2025-02-20T15:25:36.489462Z"}},"outputs":[],"execution_count":31}]}