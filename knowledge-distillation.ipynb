{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":69582,"sourceType":"modelInstanceVersion","isSourceIdPinned":true,"modelInstanceId":58059,"modelId":79606},{"sourceId":69596,"sourceType":"modelInstanceVersion","isSourceIdPinned":true,"modelInstanceId":58072,"modelId":79606},{"sourceId":104433,"sourceType":"modelInstanceVersion","isSourceIdPinned":true,"modelInstanceId":68806,"modelId":91102}],"dockerImageVersionId":30887,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# KD on Text-generation models","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19"}},{"cell_type":"code","source":"%%capture\n%pip install -U bitsandbytes\n%pip install -U accelerate\n%pip install -U peft\n%pip install -U transformers==4.48.0 #4.46.3\n%pip install -U datasets\n%pip install -U wandb","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import torch\ndevice = \"cuda\" if torch.cuda.is_available() else \"cpu\"\ntraining_bf16 = torch.cuda.is_bf16_supported()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-16T17:38:08.681615Z","iopub.execute_input":"2025-02-16T17:38:08.682032Z","iopub.status.idle":"2025-02-16T17:38:08.686880Z","shell.execute_reply.started":"2025-02-16T17:38:08.682007Z","shell.execute_reply":"2025-02-16T17:38:08.685770Z"}},"outputs":[],"execution_count":2},{"cell_type":"code","source":"# loading model\nfrom transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig\nfrom peft import LoraConfig, prepare_model_for_kbit_training, get_peft_model\n\nbnb_config = BitsAndBytesConfig(\n    load_in_4bit=True,\n    bnb_4bit_quant_type=\"nf4\",\n    bnb_4bit_use_double_quant=True,\n    bnb_4bit_compute_dtype=torch.bfloat16 if training_bf16 else torch.float16,\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-16T17:38:08.689066Z","iopub.execute_input":"2025-02-16T17:38:08.689392Z","iopub.status.idle":"2025-02-16T17:38:28.173408Z","shell.execute_reply.started":"2025-02-16T17:38:08.689364Z","shell.execute_reply":"2025-02-16T17:38:28.172677Z"}},"outputs":[],"execution_count":3},{"cell_type":"code","source":"teacher_model_path = \"JC-24/gemma-7b-mediqa-final\"\nstudent_model_path = \"/kaggle/input/gemma/transformers/2b/2\"\n\nteacher_model = AutoModelForCausalLM.from_pretrained(\n    teacher_model_path,\n    quantization_config=bnb_config\n)\n\ntokenizer = AutoTokenizer.from_pretrained(teacher_model_path)\ntokenizer.pad_token = tokenizer.eos_token","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-16T17:38:28.177618Z","iopub.execute_input":"2025-02-16T17:38:28.177965Z","iopub.status.idle":"2025-02-16T17:38:46.410957Z","shell.execute_reply.started":"2025-02-16T17:38:28.177932Z","shell.execute_reply":"2025-02-16T17:38:46.409779Z"}},"outputs":[{"name":"stderr","text":"`low_cpu_mem_usage` was None, now default to True since model is quantized.\n","output_type":"stream"}],"execution_count":4},{"cell_type":"code","source":"student_model = AutoModelForCausalLM.from_pretrained(\n    student_model_path,\n    quantization_config=bnb_config\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-16T17:38:46.412797Z","iopub.execute_input":"2025-02-16T17:38:46.413217Z","iopub.status.idle":"2025-02-16T17:38:52.856339Z","shell.execute_reply.started":"2025-02-16T17:38:46.413177Z","shell.execute_reply":"2025-02-16T17:38:52.855468Z"}},"outputs":[{"name":"stderr","text":"`low_cpu_mem_usage` was None, now default to True since model is quantized.\n","output_type":"stream"}],"execution_count":5},{"cell_type":"code","source":"# Wrap both models with DataParallel to use multiple GPUs\nstudent_model = torch.nn.DataParallel(student_model)\nteacher_model = torch.nn.DataParallel(teacher_model)\n\n# Move both models to GPU\nstudent_model = student_model.to(device)\nteacher_model = teacher_model.to(device)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-16T17:38:52.857425Z","iopub.execute_input":"2025-02-16T17:38:52.857752Z","iopub.status.idle":"2025-02-16T17:38:52.879126Z","shell.execute_reply.started":"2025-02-16T17:38:52.857716Z","shell.execute_reply":"2025-02-16T17:38:52.877949Z"}},"outputs":[],"execution_count":6},{"cell_type":"code","source":"from torch.utils.data import DataLoader\nfrom torch.optim import AdamW\n\nbatch_size = 1\nepochs = 1","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-16T17:38:52.880258Z","iopub.execute_input":"2025-02-16T17:38:52.880678Z","iopub.status.idle":"2025-02-16T17:38:52.957356Z","shell.execute_reply.started":"2025-02-16T17:38:52.880636Z","shell.execute_reply":"2025-02-16T17:38:52.956183Z"}},"outputs":[],"execution_count":7},{"cell_type":"code","source":"from torch.utils.data import Dataset\n\nclass MyDataset(Dataset):\n    def __init__(self, data):\n        self.data = data\n\n    def __len__(self):\n        return len(self.data)\n\n    def __getitem__(self, idx):\n        return self.data[idx]\n\nfrom datasets import load_dataset\n\ndata = load_dataset(\"wikitext\", \"wikitext-2-raw-v1\",split=\"train[:500]\")\n\nsplit_dataset = data.train_test_split(test_size=0.2)\n\ntrain_data = split_dataset['train']\neval_data = split_dataset['test']\n\ntrain_dataset = MyDataset(train_data)\neval_dataset = MyDataset(eval_data)\n\ntrain_dataloader = DataLoader(train_dataset, batch_size = batch_size) \neval_dataloader = DataLoader(eval_dataset, batch_size = batch_size) ","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-16T17:38:52.959225Z","iopub.execute_input":"2025-02-16T17:38:52.959589Z","iopub.status.idle":"2025-02-16T17:38:56.418676Z","shell.execute_reply.started":"2025-02-16T17:38:52.959548Z","shell.execute_reply":"2025-02-16T17:38:56.417543Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"README.md:   0%|          | 0.00/10.5k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"2dcf998ad2f74a96aea31c70e77bea90"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"test-00000-of-00001.parquet:   0%|          | 0.00/733k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"51b154bac3d641e2be0c30b6f3be681a"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"train-00000-of-00001.parquet:   0%|          | 0.00/6.36M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"7a71619b06d34459925ec12339d55429"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"validation-00000-of-00001.parquet:   0%|          | 0.00/657k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b6353b0c20bb46d6ba77cb8ba81d7d78"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating test split:   0%|          | 0/4358 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"61d796a23e924fdda363a69d68186bea"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating train split:   0%|          | 0/36718 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"bae0f24e5e20409e8b699c9348f0a462"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating validation split:   0%|          | 0/3760 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"0e37c28ae39746d9a7daa45c608faa3c"}},"metadata":{}}],"execution_count":8},{"cell_type":"code","source":"# from torchmetrics.text import Perplexity\n# from tqdm import tqdm\n# import torch.nn as nn\n\n# device = \"cuda\"\n# def calculate_perplexity_batched(model, dataloader, device):\n#     global logits,input_ids\n#     model.eval()\n#     # print(tokenizer.pad_token_id)\n#     perplexity_metric = Perplexity(ignore_index=tokenizer.pad_token_id).to(device)\n#     perp_list = []\n#     with torch.no_grad():\n#         for batch in tqdm(dataloader):\n#             tokenized_data = tokenizer(batch[\"text\"], padding=\"max_length\", max_length=512, truncation=True, return_tensors=\"pt\")\n#             input_ids = tokenized_data[\"input_ids\"].to(device)\n#             attention_mask = tokenized_data[\"attention_mask\"].to(device)\n            \n#             outputs = model(input_ids, attention_mask=attention_mask, labels=input_ids)\n#             logits = outputs.logits  # model outputs\n#             print(logits[0][0][:10])\n#             # Calculate perplexity batch-wise\n#             # logits = nn.functional.log_softmax(outputs.logits,dim=-1)\n#             perplexity_metric.update(logits[:,:-1],input_ids[:,1:])\n#             perp = perplexity_metric.compute().item()\n#             perp_list.append(perp)\n#     # Compute final perplexity\n#     final_perplexity = sum(perp_list)/len(perp_list)\n#     return final_perplexity\n\n# student_perplexity = calculate_perplexity_batched(student_model, eval_dataloader, device) # Student Perplexity: 151353.71875\n# print(\"Student Perplexity:\", student_perplexity)\n# teacher_perplexity = calculate_perplexity_batched(teacher_model, eval_dataloader, device) # Teacher Perplexity: 151384.03125\n# print(\"Teacher Perplexity:\", teacher_perplexity)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import torch.nn as nn\n\n\"\"\"\n\nwhy we calculate log_softmax for student logits and only softmax for teacher logits:\nhttps://pytorch.org/docs/stable/generated/torch.nn.KLDivLoss.html\n\n\"\"\"\n\ndef kd_loss(student_logits,teacher_logits,actual_labels, alpha=0.5, temperature=2.0):\n    \n    teacher_soft = nn.functional.softmax(teacher_logits / temperature,dim=-1)\n    student_log_soft = nn.functional.log_softmax(student_logits / temperature,dim=-1)\n    \n    kl_loss = nn.KLDivLoss(reduction = \"batchmean\")\n    kl_div_loss = kl_loss(student_log_soft,teacher_soft) * (temperature ** 2)  \n    \n    hard_loss = nn.CrossEntropyLoss()(student_logits.view(-1, student_logits.size(-1)), actual_labels.view(-1)) \n    \n    total_loss = alpha*kl_div_loss + (1-alpha)*hard_loss\n    return kl_div_loss, hard_loss, total_loss","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# \"\"\"\n# perplexity =  exp ( sum of negative log-likelihood of the tokens in the sequence / total tokens)\n# perplexity is equivalent to the exponentiation of the cross-entropy between the actual data and model predictions\n# \"\"\"\n# from tqdm import tqdm\n# # def get_perplexity(logits, target, pad_token_id):\n# #     # Convert logits to log probabilities\n# #     log_probs = torch.nn.functional.log_softmax(logits, dim=-1)\n# #     target_log_probs = log_probs.gather(dim=-1, index=target.unsqueeze(-1)).squeeze(-1)\n# #     # Calculate the negative log likelihood\n# #     mask = (target != pad_token_id)\n# #     target_log_probs = target_log_probs[mask]\n# #     negative_log_likelihood = -target_log_probs\n    \n# #     # Calculate the mean negative log likelihood over all tokens\n# #     # print(\"neg_log: \",negative_log_likelihood[0][0][:5])\n# #     mean_nll = negative_log_likelihood.mean()\n# #     # print(\"---->\",mean_nll.item())\n# #     # Calculate perplexity as exp(mean negative log likelihood)\n# #     perplexity = torch.exp(mean_nll)\n# #     # print(target[0][0], pad_token_id)\n# #     print(\"##p:\",target[0][0], pad_token_id, perplexity.item())\n# #     return perplexity.item()\n    \n# def calculate_perplexity(model, dataloader, device):\n#     losses = []\n#     with torch.no_grad():\n#         # p_list = []\n#         for data in tqdm(dataloader):\n#             tokenized_data = tokenizer(data[\"text\"],padding=\"max_length\",max_length = 512,truncation=True,return_tensors = \"pt\")\n#             input_ids = tokenized_data[\"input_ids\"].to(device)\n#             # if input_ids[0][0] == tokenizer.pad_token_id:\n#             #     continue\n#             attention_mask = tokenized_data[\"attention_mask\"].to(device)\n            \n#             outputs = model(input_ids,attention_mask=attention_mask, labels=input_ids)\n#         #     p = get_perplexity(outputs.logits,input_ids, tokenizer.pad_token_id)\n#         #     p_list.append(p)\n#         # return sum(p_list)/len(p_list)\n#             loss = outputs.loss.item()\n#             losses.append(loss)\n#     return torch.exp(torch.tensor(losses)).mean().item()\n\n\n# student_perplexity = calculate_perplexity(student_model,eval_dataloader,device)\n# print(\"student_perplexity: \",student_perplexity)\n# teacher_perplexity = calculate_perplexity(teacher_model,eval_dataloader,device)\n# print(\"teacher_perplexity: \",teacher_perplexity)\n# # student_perplexity:  9043377.0\n# # teacher_perplexity:  22564.099609375","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-16T17:49:28.252957Z","iopub.execute_input":"2025-02-16T17:49:28.253261Z","iopub.status.idle":"2025-02-16T17:49:28.257375Z","shell.execute_reply.started":"2025-02-16T17:49:28.253240Z","shell.execute_reply":"2025-02-16T17:49:28.256387Z"}},"outputs":[],"execution_count":24},{"cell_type":"code","source":"student_model = AutoModelForCausalLM.from_pretrained(\n    student_model_path,\n    quantization_config=bnb_config\n)\n\nstudent_model = prepare_model_for_kbit_training(student_model)\n\n# Define LoRA configuration for fine-tuning\npeft_config = LoraConfig(\n    task_type=\"CAUSAL_LM\",\n    inference_mode=False,\n    r=8,\n    lora_alpha=32,\n    lora_dropout=0.1\n)\n\n# Wrap student model with LoRA adapters\nstudent_model = get_peft_model(student_model, peft_config)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"optimizer = AdamW(student_model.parameters(),lr = 1.4e-5)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#train_loop\nteacher_model.eval()\nlogging_step = 50\nfor epoch in range(epochs):\n    for step, data in enumerate(train_dataloader):\n        \n        tokenized_data = tokenizer(data[\"text\"],padding=\"max_length\",max_length = 512,truncation=True,return_tensors = \"pt\")\n        input_ids = tokenized_data[\"input_ids\"].to(device)\n        attention_mask = tokenized_data[\"attention_mask\"].to(device)\n        with torch.no_grad():\n            teacher_out = teacher_model(input_ids,attention_mask=attention_mask)\n            \n        student_out = student_model(input_ids,attention_mask=attention_mask)\n        student_logits = student_out.logits\n        kl_div_loss, hard_loss, total_loss = kd_loss(student_out.logits,teacher_out.logits, input_ids)\n\n        if step%logging_step==0:\n            print(\"step: \",step,\" kl_div_loss:\",kl_div_loss.item(), \" hard_loss:\",hard_loss.item(), \" total_loss:\", total_loss.item())\n        optimizer.zero_grad()\n        total_loss.backward()\n        optimizer.step()\n\n    student_model.eval()\n    student_perplexity = calculate_perplexity(student_model,eval_dataloader,device)\n    teacher_perplexity = calculate_perplexity(teacher_model,eval_dataloader,device)\n    print(\"Perplexity of student model: \", student_perplexity)\n    print(\"Perplexity of teacher model: \", teacher_perplexity)\n    #evaluate the model\n    student_model.train()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# KD using Tranformers Trainer","metadata":{}},{"cell_type":"code","source":"%%capture\n%pip install -U bitsandbytes\n%pip install -U accelerate\n%pip install -U peft\n%pip install -U transformers==4.48.0 #4.46.3\n%pip install -U datasets\n%pip install -U wandb","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from huggingface_hub import login\nfrom kaggle_secrets import UserSecretsClient\nuser_secrets = UserSecretsClient()\n\nhf_token = user_secrets.get_secret(\"HUGGINGFACE_TOKEN\")\nlogin(token = hf_token)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import torch\ntraining_bf16 = torch.cuda.is_bf16_supported()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# loading model\nfrom transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig\nfrom peft import LoraConfig, prepare_model_for_kbit_training, get_peft_model\n\nbnb_config = BitsAndBytesConfig(\n    load_in_4bit=True,\n    bnb_4bit_quant_type=\"nf4\",\n    bnb_4bit_use_double_quant=True,\n    bnb_4bit_compute_dtype=torch.bfloat16 if training_bf16 else torch.float16,\n)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"teacher_model_path = \"/kaggle/input/qwen2/transformers/1.5b/1\"\nstudent_model_path = \"/kaggle/input/qwen2/transformers/0.5b/1\"\n\nteacher_model = AutoModelForCausalLM.from_pretrained(\n    teacher_model_path,\n    quantization_config=bnb_config\n)\n\ntokenizer = AutoTokenizer.from_pretrained(teacher_model_path)\ntokenizer.pad_token = tokenizer.eos_token\n\nstudent_model = AutoModelForCausalLM.from_pretrained(\n    student_model_path,\n    quantization_config=bnb_config\n)\n\nstudent_model = prepare_model_for_kbit_training(student_model)\n\n# Define LoRA configuration for fine-tuning\npeft_config = LoraConfig(\n    task_type=\"CAUSAL_LM\",\n    inference_mode=False,\n    r=8,\n    lora_alpha=32,\n    lora_dropout=0.1\n)\n\n# Wrap student model with LoRA adapters\nstudent_model = get_peft_model(student_model, peft_config)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from datasets import load_dataset\ndata = load_dataset(\"wikitext\", \"wikitext-2-raw-v1\",split=\"train[:500]\")\n\nsplit_dataset = data.train_test_split(test_size=0.2)\n\ntrain_data = split_dataset['train']\neval_data = split_dataset['test']","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from transformers import Trainer, TrainingArguments, AutoModelForCausalLM, AutoTokenizer\n\n\n# Define training arguments\ntraining_args = TrainingArguments(\n    output_dir=\"./results\",\n    evaluation_strategy=\"steps\",\n    eval_steps = 0.2,\n    max_steps=1,\n    learning_rate=5e-5,\n    per_device_train_batch_size=1,\n    per_device_eval_batch_size=1,\n    # num_train_epochs=1,\n    weight_decay=0.01,\n    logging_dir=\"./logs\",\n    logging_steps=1,\n    save_strategy=\"steps\",\n    remove_unused_columns=False\n)\n\n# Custom DataCollator for Distillation\nfrom transformers import DataCollatorForSeq2Seq\ndata_collator = DataCollatorForSeq2Seq(tokenizer, model=student_model)\n\n# Trainer for KD\nclass DistillationTrainer(Trainer):\n    def __init__(self, *args, teacher_model=None, temperature=2.0, alpha=0.5, **kwargs):\n        super().__init__(*args, **kwargs)\n        self.teacher_model = teacher_model\n        self.temperature = temperature\n        self.alpha = alpha\n\n    def compute_loss(self, model, inputs, return_outputs=False):\n        # Forward pass for student\n        print(\"---------------------------\")\n        inputs = tokenizer(inputs[\"text\"],padding=\"max_length\",max_length = 512,truncation=True,return_tensors = \"pt\")\n        labels = inputs.pop(\"labels\")\n        inputs.pop(\"text\")\n        outputs_student = model(**inputs)\n        logits_student = outputs_student.logits\n\n        # Forward pass for teacher\n        with torch.no_grad():\n            outputs_teacher = self.teacher_model(**inputs)\n            logits_teacher = outputs_teacher.logits\n\n        # Compute KD loss\n        loss_kl = F.kl_div(\n            F.log_softmax(logits_student / self.temperature, dim=-1),\n            F.softmax(logits_teacher / self.temperature, dim=-1),\n            reduction=\"batchmean\"\n        ) * (self.temperature ** 2)\n\n        # Compute hard label loss\n        loss_ce = F.cross_entropy(logits_student.view(-1, logits_student.size(-1)), labels.view(-1))\n\n        # Total loss\n        loss = self.alpha * loss_kl + (1 - self.alpha) * loss_ce\n        print(loss,\"----------------------------\")\n        return (loss, outputs_student) if return_outputs else loss\n\n# Trainer instance\ntrainer = DistillationTrainer(\n    model=student_model,\n    args=training_args,\n    train_dataset=train_data,\n    eval_dataset=eval_data,\n    tokenizer=tokenizer,\n    data_collator=data_collator,\n    teacher_model=teacher_model,\n    temperature=2.0,\n    alpha=0.5\n)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Train\ntrainer.train()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# KD on Sequence Classification Model","metadata":{}},{"cell_type":"code","source":"%%capture\n%pip install -U transformers==4.48.0 #4.46.3\n%pip install -U datasets","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from datasets import load_dataset\n\nfrom transformers import AutoTokenizer, AutoModelForSequenceClassification\nfrom transformers import DistilBertForSequenceClassification, DistilBertConfig\n\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torch.utils.data import DataLoader\n\nfrom sklearn.metrics import accuracy_score, precision_recall_fscore_support","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from huggingface_hub import login\nfrom kaggle_secrets import UserSecretsClient\nuser_secrets = UserSecretsClient()\n\nhf_token = user_secrets.get_secret(\"HUGGINGFACE_TOKEN\")\nlogin(token = hf_token)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"device = torch.device('cuda')\ndevice","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Load teacher model and tokenizer\nmodel_path = \"shawhin/bert-phishing-classifier_teacher\"\n\ntokenizer = AutoTokenizer.from_pretrained(model_path)\nteacher_model = AutoModelForSequenceClassification.from_pretrained(\n    model_path).to(device)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Load student model\nmy_config = DistilBertConfig(n_heads=8, n_layers=4) # drop 4 heads per layer and 2 layers\nstudent_model = DistilBertForSequenceClassification.from_pretrained(\"distilbert-base-uncased\",\n                                                                    config=my_config).to(device)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"data = load_dataset(\"shawhin/phishing-site-classification\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# define text preprocessing\ndef preprocess_function(examples):\n    return tokenizer(examples[\"text\"], padding='max_length', truncation=True)\n\n# tokenize all datasetse\ntokenized_data = data.map(preprocess_function, batched=True)\ntokenized_data.set_format(type='torch', columns=['input_ids', 'attention_mask', 'labels'])","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def evaluate_model(model, dataloader, device):\n    model.eval()  # Set model to evaluation mode\n    all_preds = []\n    all_labels = []\n\n    # Disable gradient calculations\n    with torch.no_grad():\n        for batch in dataloader:\n            input_ids = batch['input_ids'].to(device)\n            attention_mask = batch['attention_mask'].to(device)\n            labels = batch['labels'].to(device)\n\n            # Forward pass to get logits\n            outputs = model(input_ids, attention_mask=attention_mask)\n            logits = outputs.logits\n\n            # Get predictions\n            preds = torch.argmax(logits, dim=1).cpu().numpy()\n            all_preds.extend(preds)\n            all_labels.extend(labels.cpu().numpy())\n\n    # Calculate evaluation metrics\n    accuracy = accuracy_score(all_labels, all_preds)\n    precision, recall, f1, _ = precision_recall_fscore_support(all_labels, all_preds, average='binary')\n\n    return accuracy, precision, recall, f1","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Function to compute distillation and hard-label loss\ndef distillation_loss(student_logits, teacher_logits, true_labels, temperature, alpha):\n    # Compute soft targets from teacher logits\n    soft_targets = nn.functional.softmax(teacher_logits / temperature, dim=1)\n    student_soft = nn.functional.log_softmax(student_logits / temperature, dim=1)\n\n    # KL Divergence loss for distillation\n    distill_loss = nn.functional.kl_div(student_soft, soft_targets, reduction='batchmean') * (temperature ** 2)\n\n    # Cross-entropy loss for hard labels\n    hard_loss = nn.CrossEntropyLoss()(student_logits, true_labels)\n\n    # Combine losses\n    loss = alpha * distill_loss + (1.0 - alpha) * hard_loss\n\n    return loss","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# hyperparameters\nbatch_size = 32\nlr = 1e-4\nnum_epochs = 5\ntemperature = 2.0\nalpha = 0.5\n\n# define optimizer\noptimizer = optim.Adam(student_model.parameters(), lr=lr)\n\n# create training data loader\ndataloader = DataLoader(tokenized_data['train'], batch_size=batch_size)\n# create testing data loader\ntest_dataloader = DataLoader(tokenized_data['test'], batch_size=batch_size)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# put student model in train mode\nstudent_model.train()\nteacher_model.eval()\n# train model\nfor epoch in range(num_epochs):\n    for batch in dataloader:\n        # Prepare inputs\n        input_ids = batch['input_ids'].to(device)\n        attention_mask = batch['attention_mask'].to(device)\n        labels = batch['labels'].to(device)\n\n        # Disable gradient calculation for teacher model\n        with torch.no_grad():\n            teacher_outputs = teacher_model(input_ids, attention_mask=attention_mask)\n            teacher_logits = teacher_outputs.logits\n\n        # Forward pass through the student model\n        student_outputs = student_model(input_ids, attention_mask=attention_mask)\n        student_logits = student_outputs.logits\n        # Compute the distillation loss\n        loss = distillation_loss(student_logits, teacher_logits, labels, temperature, alpha)\n\n        # Backpropagation\n        optimizer.zero_grad()\n        loss.backward()\n        optimizer.step()\n\n    print(f\"Epoch {epoch + 1} completed with loss: {loss.item()}\")\n\n    # Evaluate the teacher model\n    teacher_accuracy, teacher_precision, teacher_recall, teacher_f1 = evaluate_model(teacher_model, test_dataloader, device)\n    print(f\"Teacher (test) - Accuracy: {teacher_accuracy:.4f}, Precision: {teacher_precision:.4f}, Recall: {teacher_recall:.4f}, F1 Score: {teacher_f1:.4f}\")\n\n    # Evaluate the student model\n    student_accuracy, student_precision, student_recall, student_f1 = evaluate_model(student_model, test_dataloader, device)\n    print(f\"Student (test) - Accuracy: {student_accuracy:.4f}, Precision: {student_precision:.4f}, Recall: {student_recall:.4f}, F1 Score: {student_f1:.4f}\")\n    print(\"\\n\")\n\n    # put student model back into train mode\n    student_model.train()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"student_model.push_to_hub(\"shawhin/bert-phishing-classifier_student\")","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}